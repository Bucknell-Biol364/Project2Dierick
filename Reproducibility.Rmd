---
title: "Reproducibility"
author: "Jordan and Matt"
date: "3/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

So we found two main papers to focus on for reproducibility. The first one is a review(kind of) of repeatability of published microarray gene expression analyses. This paper (labeled "out.pdf") in our repo, looked at 18 different papers, with the Dierick paper listed at number 16. The figure that was randomly selected was table 2, and the reproducibility stated that the raw data was not given, but the processed data was. Overall, the review paper determined that table 2 was able to be reproduced, with some discrepancies. They stated things like, "expression fold change values being inconsistant". What was really interesting about this paper was that one team of analysts for the Dierick paper table 2 described it was "cannot reproduce", while another team described it as "can reproduce partially with some discrepancies", so it's interesting that there seemed to be a pretty significant issue with that data/the table that was produced. I would've liked them to try figure 2, which is what we had such a big problem with (potential p hacking) - but the assignemnt of figures/tables to reproduce was randomized.

